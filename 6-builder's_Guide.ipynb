{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3270e750",
      "metadata": {
        "id": "3270e750"
      },
      "source": [
        "# Builders’ Guide\n",
        "Details of the APIs: how they work. Key components of deep learning computation, namely model construction, parameter access and initialization, designing custom layers and blocks, reading and writing models to disk, and leveraging GPUs to achieve dramatic speedups.\n",
        "## 6.1. Layers and Modules\n",
        "Linear models with a single output neural network, consist of a single neuron:\n",
        "1. takes some set of inputs\n",
        "2. generates a corresponding scalar output\n",
        "3. has a set of associated parameters that can be updated to optimize some objective function of interest.\n",
        "\n",
        "Networks with multiple outputs / layers:\n",
        "1. take a set of inputs\n",
        "2. generate corresponding outputs\n",
        "3. described by a set of tunable parameters.\n",
        "\n",
        "For MLPs and its layers:\n",
        "1. take raw inputs/features\n",
        "2. generate outputs/predictions\n",
        "3. possesses parameters/combined parameters form all constituent layers\n",
        "\n",
        "Neural network **module** for implement the complex networks. It can be a single layer, a component consisting of multiple layers, or the model.   \n",
        "A module is represented by a **class**, an subclass of it must define a forward propagation method (transforms input into output, store necessary parameters), and process a backpropagation method (calculate gradients, not need to worry).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac63b2f",
      "metadata": {
        "id": "8ac63b2f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00efc141",
      "metadata": {
        "id": "00efc141"
      },
      "source": [
        "Implement a network with 1 fully connected hidden layer, 256 units and ReLU activation, and a fully connected output layer, 10 units, no activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1a1c05",
      "metadata": {
        "id": "5f1a1c05",
        "outputId": "02ed829e-5d2e-4730-eeeb-3d699d454159"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "\n",
        "X = torch.rand(2, 20)\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c506f5",
      "metadata": {
        "id": "79c506f5"
      },
      "source": [
        "Construct model by instantiating an ```nn.Sequential```, defines a special kind of ```Module```. Each 2 fully connected layers is an instance of the ```Linear``` class which is a subclass of ```Module```.  \n",
        "The ```forward``` propagation method chains each module in the list and pass the output of each as input to the next.  \n",
        "Invoke models via ```net(X)```  (a shorthand for ```net.__call__(X)```) to obtain output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0eb63f",
      "metadata": {
        "id": "df0eb63f"
      },
      "source": [
        "### 6.1.1. A Custom Module\n",
        "Basic functionality that each module must provide:\n",
        "1. Ingest input data as arguments to its forward propagation method.\n",
        "2. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input.\n",
        "3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method.\n",
        "4. Store and provide access to those parameters necessary for executing the forward propagation computation.\n",
        "5. Initialize model parameters as needed.\n",
        "\n",
        "Code a module for an MLP with 1 hidden layer with 256 hidden units and a 10-dimensional output layer. Supply only ```__init__``` method  and the forward propagation method.  \n",
        "- in ```forward``` method: input X, calculate hidden representation + activation applied, output the logits.\n",
        "- ```__init__```invokes the parent's init via ```super().__init__()```\n",
        "- instantiate 2 fully connected layers, assigning to ```self.hidden```, ```self.out```. System will generate backpropagation automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fb013c",
      "metadata": {
        "id": "42fb013c"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Call the constructor of the parent class nn.Module to perform\n",
        "        # the necessary initialization\n",
        "        super().__init__()\n",
        "        self.hidden = nn.LazyLinear(256)\n",
        "        self.out = nn.LazyLinear(10)\n",
        "\n",
        "    # Define the forward propagation of the model, that is, how to return the\n",
        "    # required model output based on the input X\n",
        "    def forward(self, X):\n",
        "        return self.out(F.relu(self.hidden(X)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53b4fb6",
      "metadata": {
        "id": "e53b4fb6",
        "outputId": "69723ac1-116e-4ee4-ed46-5efdd5195b17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = MLP()\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ccaf6c",
      "metadata": {
        "id": "06ccaf6c"
      },
      "source": [
        "### 6.1.2. The Sequential Module\n",
        "```Sequential``` is used to daisy-chain other modules together. For our simplified sequential we need 2 methods:\n",
        "1. A method for appending modules one by one to a list.\n",
        "2. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended.\n",
        "\n",
        "- in ```__init__```: add module by calling ```add_modules``` method.\n",
        "- in ```forward```propagation method, each added module is executedin the order they added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e6fb0a",
      "metadata": {
        "id": "35e6fb0a"
      },
      "outputs": [],
      "source": [
        "# same functionality of the default Sequential\n",
        "class MySequential(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        for idx, module in enumerate(args):\n",
        "            self.add_module(str(idx), module)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for module in self.children():\n",
        "            X = module(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef93e96",
      "metadata": {
        "id": "6ef93e96",
        "outputId": "4b8fe27f-2e49-4f7a-f448-96ab961b3e7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad29863",
      "metadata": {
        "id": "0ad29863"
      },
      "source": [
        "### 6.1.3. Executing Code in the Forward Propagation Method\n",
        "Not all architectures are simple daisy chains. We aim to perform arbitrary mathematical operations.  \n",
        "So far all operations act on network's activation and its parameters. We also want to act on terms that neigher the output of previous layer nor updatable parameters, which is called **constant parameters**. We implement a ```FixedHiddenMLP``` class as follows:\n",
        "- weights are initialized randomly, which is not a model parameter and never updated by backpropagation.\n",
        "- run a while-loop to ensure the output's $\\ell_1$ norm is smaller than 1, if not, divide output by 2.\n",
        "- return the sum of the entiries in X.\n",
        "\n",
        "Above may not useful but just for showing how to use arbitrary code into NN flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b1c668",
      "metadata": {
        "id": "85b1c668"
      },
      "outputs": [],
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Random weight parameters that will not compute gradients and\n",
        "        # therefore keep constant during training\n",
        "        self.rand_weight = torch.rand((20, 20))\n",
        "        self.linear = nn.LazyLinear(20)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.linear(X)\n",
        "        X = F.relu(X @ self.rand_weight + 1)\n",
        "        # Reuse the fully connected layer. This is equivalent to sharing\n",
        "        # parameters with two fully connected layers\n",
        "        X = self.linear(X)\n",
        "        # Control flow\n",
        "        while X.abs().sum() > 1:\n",
        "            X /= 2\n",
        "        return X.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478bb849",
      "metadata": {
        "id": "478bb849",
        "outputId": "cecf66fb-e1ba-47df-db04-4b177f4677b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.0721, grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35aaad82",
      "metadata": {
        "id": "35aaad82",
        "outputId": "1df48a8b-01c7-41ee-a94b-a67020872e51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3910, grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# mix modules together example\n",
        "class NestMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
        "                                 nn.LazyLinear(32), nn.ReLU())\n",
        "        self.linear = nn.LazyLinear(16)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.linear(self.net(X))\n",
        "\n",
        "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
        "chimera(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a55df3",
      "metadata": {
        "id": "b1a55df3"
      },
      "source": [
        "### 6.1.4. Summary\n",
        "- Individual layers can be modules. Many layers can comprise a module. Many modules can comprise a module.\n",
        "- a Module can contain code. Modules can do parameter initialization and backpropagation.\n",
        "- Use ```Sequential``` to handle sequential concatenations of layers and modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ed07c5",
      "metadata": {
        "id": "38ed07c5"
      },
      "source": [
        "## 6.2. Parameter Management\n",
        "- Accessing parameters for debugging, diagnostics, and visualizations.\n",
        "- Sharing parameters across different model components.\n",
        "\n",
        "### 6.2.1. Parameter Access\n",
        "When a model is defined via ```Sequential``` class: access layer by indexing. Each layer's parameters are in the attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4b1450",
      "metadata": {
        "id": "bf4b1450",
        "outputId": "622a88fe-6263-4ce4-9fe6-549d5a29d1de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "net = nn.Sequential(nn.LazyLinear(8),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1557f81",
      "metadata": {
        "id": "a1557f81",
        "outputId": "b5d79c3d-c0cb-4a8a-9855-802a442b1b4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[-0.1904, -0.0517, -0.1419, -0.3515, -0.2102, -0.0083,  0.2476,  0.0567]])),\n",
              "             ('bias', tensor([0.3388]))])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net[2].state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b613bc",
      "metadata": {
        "id": "86b613bc"
      },
      "source": [
        "#### 6.2.1.1. Targeted Parameters\n",
        "Each parameter is represented as an instance of the parameter class. To access the underlying numerical values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd39d1d",
      "metadata": {
        "id": "bcd39d1d",
        "outputId": "af9aedb8-e8e6-4ab3-a890-983164967859"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.nn.parameter.Parameter, tensor([0.3388]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extract bias from 2nd NN layer, return a parameter class instance\n",
        "type(net[2].bias), net[2].bias.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bfec0d",
      "metadata": {
        "id": "23bfec0d",
        "outputId": "ec160417-1721-4e33-c156-eeda6926f0d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# parameter object contains value, gradient, and additional info\n",
        "net[2].weight.grad == None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b535ab",
      "metadata": {
        "id": "e9b535ab"
      },
      "source": [
        "#### 6.2.1.2. All Parameters at Once\n",
        "Access parameters of all layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b6247b",
      "metadata": {
        "id": "f7b6247b",
        "outputId": "ce6b0347-cc44-45c9-ca61-b179f6f26de5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('0.weight', torch.Size([8, 4])),\n",
              " ('0.bias', torch.Size([8])),\n",
              " ('2.weight', torch.Size([1, 8])),\n",
              " ('2.bias', torch.Size([1]))]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(name, param.shape) for name, param in net.named_parameters()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d6dba8",
      "metadata": {
        "id": "43d6dba8"
      },
      "source": [
        "### 6.2.2. Tied Parameters\n",
        "Share parameters across multiple layers. Parameters between layer are tied and represented by the same tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5638a872",
      "metadata": {
        "id": "5638a872",
        "outputId": "6fbb2a55-58bb-4d8c-cca0-34fe9aa9fae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([True, True, True, True, True, True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True])\n"
          ]
        }
      ],
      "source": [
        "# We need to give the shared layer a name so that we can refer to its parameters\n",
        "shared = nn.LazyLinear(8)\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "net(X)\n",
        "# Check whether the parameters are the same\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
        "net[2].weight.data[0, 0] = 100\n",
        "# Make sure that they are actually the same object rather than just having the same value\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45af9b00",
      "metadata": {
        "id": "45af9b00"
      },
      "source": [
        "### 6.2.3 Summary\n",
        "Ways to access model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92892bd1",
      "metadata": {
        "id": "92892bd1"
      },
      "source": [
        "## 6.3. Parameter Initialization\n",
        "How to initialize parameters properly:  \n",
        "Default is weight/bias are uniformly drawn from a range computed from the input & output dimension. ```nn.init``` module provides preset initialization methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b6aa78",
      "metadata": {
        "id": "d2b6aa78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0552c975",
      "metadata": {
        "id": "0552c975",
        "outputId": "2aaa3d96-5241-47a1-ad4b-e90c3ff76875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502f74be",
      "metadata": {
        "id": "502f74be"
      },
      "source": [
        "### 6.3.1. Built-in Initialization\n",
        "Initialize all weight parameters as Gaussian random variables with standard deviation = 0.01, bias = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea083eb0",
      "metadata": {
        "id": "ea083eb0",
        "outputId": "cd831439-d196-4062-c12a-703129df4560"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 0.0032, -0.0064, -0.0138, -0.0060]), tensor(0.))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_normal(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "net.apply(init_normal)\n",
        "net[0].weight.data[0], net[0].bias.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2036da99",
      "metadata": {
        "id": "2036da99",
        "outputId": "968cea87-f3c5-489d-a962-dd64c2f8f66b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1.]), tensor(0.))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize all parameter to a given constant\n",
        "def init_constant(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "net.apply(init_constant)\n",
        "net[0].weight.data[0], net[0].bias.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce0ce2a",
      "metadata": {
        "id": "2ce0ce2a",
        "outputId": "4d3bda33-652f-48ab-8b26-2f9d584f3a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.0935, -0.3470, -0.3493,  0.4180])\n",
            "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
          ]
        }
      ],
      "source": [
        "# apply different initializers for certain blocks.\n",
        "def init_xavier(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "def init_42(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.constant_(module.weight, 42)\n",
        "\n",
        "# initialize 1st layer with Xavier initializer\n",
        "net[0].apply(init_xavier)\n",
        "# initialize 2nd layer to constant 42\n",
        "net[2].apply(init_42)\n",
        "print(net[0].weight.data[0])\n",
        "print(net[2].weight.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135278dc",
      "metadata": {
        "id": "135278dc"
      },
      "source": [
        "#### 6.3.1.1. Custom Initialization\n",
        "For initialization methods that are not provided by the framwork, define a function for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f2c5db",
      "metadata": {
        "id": "f0f2c5db",
        "outputId": "06f53798-c8c5-4bba-8385-437acf6204a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init weight torch.Size([8, 4])\n",
            "Init weight torch.Size([1, 8])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-7.1898, -0.0000, -0.0000,  0.0000],\n",
              "        [ 6.6734, -5.5504, -0.0000, -7.9045]], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# U(5,10) with p=1/4, 0 with p=1/2, U(-10,-5) with p=1/4\n",
        "def my_init(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        print(\"Init\", *[(name, param.shape)\n",
        "                        for name, param in module.named_parameters()][0])\n",
        "        nn.init.uniform_(module.weight, -10, 10)\n",
        "        module.weight.data *= module.weight.data.abs() >= 5\n",
        "\n",
        "net.apply(my_init)\n",
        "net[0].weight[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62f5f3d",
      "metadata": {
        "id": "f62f5f3d",
        "outputId": "53f85e65-f8a5-42d8-c1df-1dcf7686b011"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([42.,  1.,  1.,  1.])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set parameters directly\n",
        "net[0].weight.data[:] += 1\n",
        "net[0].weight.data[0, 0] = 42\n",
        "net[0].weight.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c75f7d",
      "metadata": {
        "id": "e8c75f7d"
      },
      "source": [
        "### 6.3.2 Summary\n",
        "We can initialize parameters using built-in and custom initializers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7e9c1e",
      "metadata": {
        "id": "0d7e9c1e"
      },
      "source": [
        "## 6.4. Lazy Initialization\n",
        "For the unintuitive things: define the network architectures without specifying the input dimensionality, add layers without specifying the output dimension of the previous layer, initialize parameters before providing enough info to determine how many parameters we need.  \n",
        "The framework **defers initialization**, waiting until the first time we pass data through the model, to infer the sizes of each layer on the fly. Next, we go deeper into the mechanics of initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0001fd08",
      "metadata": {
        "id": "0001fd08"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee04169",
      "metadata": {
        "id": "4ee04169",
        "outputId": "cf9e032e-a782-4d82-c168-b8320ffe2231"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<UninitializedParameter>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# instantiate an MLP\n",
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "\n",
        "# the framework has not initialized any parameters.\n",
        "net[0].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae06cc2b",
      "metadata": {
        "id": "ae06cc2b",
        "outputId": "de51553a-97c5-4a23-ee47-63e142f27d28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 20])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pass data through then the framwork initialize parameters.\n",
        "X = torch.rand(2, 20)\n",
        "net(X)\n",
        "\n",
        "net[0].weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a421fd9d",
      "metadata": {
        "id": "a421fd9d"
      },
      "source": [
        "With input dimensionality, the framework can find the shape of the first layer. With the shape of first layer, the framework compute the computational graph so all shapes are known. Only the first layer requires lazy initialization, the framework initializes sequentially. Once all shapes are known, the framework initialize the parameters.   \n",
        "The following method passes in dummy inputs through the network for a dry run to infer all parameter shapes and subsequently initializes the parameters. It will be used later when default random initializations are not desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71079648",
      "metadata": {
        "id": "71079648"
      },
      "outputs": [],
      "source": [
        "# dry run for better initialization.\n",
        "@d2l.add_to_class(d2l.Module)  #@save\n",
        "def apply_init(self, inputs, init=None):\n",
        "    self.forward(*inputs)\n",
        "    if init is not None:\n",
        "        self.net.apply(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6691612",
      "metadata": {
        "id": "c6691612"
      },
      "source": [
        "### 6.4.1 Summary\n",
        "Lazy initialization can be convenient, allowing the framework to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors. We can pass data through the model to make the framework finally initialize parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b898d901",
      "metadata": {
        "id": "b898d901"
      },
      "source": [
        "## 6.5. Custom Layers\n",
        "### 6.5.1. Layers without Parameters\n",
        "Construct a custom layer that does not have any parameters of its own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ee0846",
      "metadata": {
        "id": "c5ee0846"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e97914",
      "metadata": {
        "id": "82e97914"
      },
      "outputs": [],
      "source": [
        "class CenteredLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X - X.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8da8aaa",
      "metadata": {
        "id": "b8da8aaa",
        "outputId": "74eae5ae-9510-4af6-9c12-d792c0acf230"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-2., -1.,  0.,  1.,  2.])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example of usage\n",
        "layer = CenteredLayer()\n",
        "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee5bbb8e",
      "metadata": {
        "id": "ee5bbb8e"
      },
      "outputs": [],
      "source": [
        "# use as a component in complex models\n",
        "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39bc247",
      "metadata": {
        "id": "e39bc247",
        "outputId": "509a52dc-0b94-465a-f2e3-a061cf87a932"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(5.5879e-09, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example of model usage, mean should be 0\n",
        "Y = net(torch.rand(4, 8))\n",
        "Y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b55bb6",
      "metadata": {
        "id": "52b55bb6"
      },
      "source": [
        "### 6.5.2. Layers with Parameters\n",
        "Implement a fully connected layer with\n",
        "- 2 parameters: weight and bias\n",
        "- 2 input arguments: in_units and units, denoted # inputs and # outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db5caa08",
      "metadata": {
        "id": "db5caa08"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_units, units):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
        "        self.bias = nn.Parameter(torch.randn(units,))\n",
        "\n",
        "    def forward(self, X):\n",
        "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
        "        return F.relu(linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fd224f",
      "metadata": {
        "id": "b4fd224f",
        "outputId": "1765db96-3543-4f95-928e-378153e5ded0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.0234, -0.0247,  2.1286],\n",
              "        [ 0.1858, -0.2367, -1.3932],\n",
              "        [ 1.7267,  1.1290, -0.4259],\n",
              "        [-0.0433,  0.5845, -0.0928],\n",
              "        [-0.4610,  0.1135,  0.5796]], requires_grad=True)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# instantiate the MyLinear class\n",
        "linear = MyLinear(5, 3)\n",
        "# access model parameters\n",
        "linear.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9367bab1",
      "metadata": {
        "id": "9367bab1",
        "outputId": "cbba4a0f-028d-4eed-d2ad-8062ae31fa12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7767, 0.0470, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000]])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# forward propagation calculations using custom layers\n",
        "linear(torch.rand(2, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8566a52",
      "metadata": {
        "id": "c8566a52",
        "outputId": "f447447e-72c7-4aa2-d8f7-564f2656c5b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[6.6446],\n",
              "        [0.0000]])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# models using custom layers\n",
        "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
        "net(torch.rand(2, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65eab5a0",
      "metadata": {
        "id": "65eab5a0"
      },
      "source": [
        "### 6.5.3 Summary\n",
        "Design custom layers via the layer class, which allows us to define flexible new layers. Layers can have local parameters, which can be created through build-in functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5442205",
      "metadata": {
        "id": "d5442205"
      },
      "source": [
        "## 6.6. File I/O\n",
        "Save the learned models or when running a long training process, periodically save intermediate results.\n",
        "### 6.6.1. Loading and Saving Tensors¶\n",
        "Invoke ```load``` and ```save``` functions to read and write individual tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f1d361",
      "metadata": {
        "id": "b1f1d361",
        "outputId": "d44ea379-d00e-48fc-fb4a-4dc9e240a4d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(4)\n",
        "torch.save(x, 'x-file')\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a020a978",
      "metadata": {
        "id": "a020a978",
        "outputId": "21a46238-2444-45f0-edfd-dba0956eb1c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\coco\\AppData\\Local\\Temp\\ipykernel_21700\\2392936229.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  x2 = torch.load('x-file')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read the data from the file\n",
        "x2 = torch.load('x-file')\n",
        "x2 == x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da1ccf4",
      "metadata": {
        "id": "6da1ccf4",
        "outputId": "875e3b4d-400f-477c-9f62-2ac9eef8d275"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\coco\\AppData\\Local\\Temp\\ipykernel_21700\\1934923035.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  x2, y2 = torch.load('x-files')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# store a list of tensors and read back\n",
        "y = torch.zeros(4)\n",
        "torch.save([x, y],'x-files')\n",
        "x2, y2 = torch.load('x-files')\n",
        "(x2, y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec37b9e8",
      "metadata": {
        "id": "ec37b9e8",
        "outputId": "ecb9b399-1bc7-4a3b-c520-797ff78b4362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\coco\\AppData\\Local\\Temp\\ipykernel_21700\\1129133509.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mydict2 = torch.load('mydict')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# write and read a dictionary that maps from strings to tensors\n",
        "mydict = {'x': x, 'y': y}\n",
        "torch.save(mydict, 'mydict')\n",
        "mydict2 = torch.load('mydict')\n",
        "mydict2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725f0f1e",
      "metadata": {
        "id": "725f0f1e"
      },
      "source": [
        "### 6.6.2. Loading and Saving Model Parameters\n",
        "Load and save entire networks using build-in functionalities. Save the **parameters** and not the entire model. The architecture need to be specified separately.  \n",
        "To reinstate a model, we need to generate the architecture in code then load the parameters from disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff59809b",
      "metadata": {
        "id": "ff59809b"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.LazyLinear(256)\n",
        "        self.output = nn.LazyLinear(10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.output(F.relu(self.hidden(x)))\n",
        "\n",
        "net = MLP()\n",
        "X = torch.randn(size=(2, 20))\n",
        "Y = net(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7b38ba",
      "metadata": {
        "id": "af7b38ba"
      },
      "outputs": [],
      "source": [
        "# store the parameters as a file\n",
        "torch.save(net.state_dict(), 'mlp.params')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee2c0b9",
      "metadata": {
        "id": "fee2c0b9",
        "outputId": "b4d997df-a05b-4b74-bcc8-aa57a7c33b92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\coco\\AppData\\Local\\Temp\\ipykernel_21700\\150697765.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  clone.load_state_dict(torch.load('mlp.params'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# recover the model\n",
        "# instantiate the original MLP model\n",
        "clone = MLP()\n",
        "# read parameters from the file\n",
        "clone.load_state_dict(torch.load('mlp.params'))\n",
        "clone.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c12f0d",
      "metadata": {
        "id": "12c12f0d",
        "outputId": "287f38fa-d0e7-4ffd-f7e7-bdaa7971f5cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True, True, True, True, True]])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# verify if is the same\n",
        "Y_clone = clone(X)\n",
        "Y_clone == Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56aaa96b",
      "metadata": {
        "id": "56aaa96b"
      },
      "source": [
        "### 6.6.3 Summary\n",
        "- ```save``` and ```load``` functions can be used for file I/O for tensors.\n",
        "- save and load the parameters of a network via a parameter dictionary\n",
        "- save the architecture has to be done in code rather than in parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96827c11",
      "metadata": {
        "id": "96827c11"
      },
      "source": [
        "## 6.7 GPUs\n",
        "How to use a single NVIDIA GPU for calculations.  \n",
        "In PyTorch, every array has a device; we often refer it as a **context**. By default, all variables and associated computation have been assigned to the CPU. Other contexts might be various GPUs.  \n",
        "By assigning arrays to contexts intelligently, we can minimize the time spent transferring data between devices. For example, when training neural networks on a server with a GPU, we typically prefer for the model’s parameters to live on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d388c17b",
      "metadata": {
        "id": "d388c17b"
      },
      "outputs": [],
      "source": [
        "# use google colab\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed886931",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed886931",
        "outputId": "50487eac-f585-4106-cb28-b98ecd70ac5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping d2l as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting d2l\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: d2l\n",
            "Successfully installed d2l-1.0.3\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -y d2l\n",
        "%pip install --no-deps d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "baa0c50b",
      "metadata": {
        "id": "baa0c50b"
      },
      "outputs": [],
      "source": [
        "import d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0a1fda",
      "metadata": {
        "id": "4c0a1fda"
      },
      "source": [
        "### 6.7.1. Computing Devices\n",
        "Specify devices for storage and calculation. By default the tensors are created in the main memory and then CPU is used for calculation. CPU and GPU can be indicated by ```torch.device('cpu)``` and ```torch.device('cuda')```. ```cpu``` means all physical CPUs and memory. ```gpu```represents one card and the corresponding memory. ```torch.device(f'cuda:{i}')``` to represent the $i^{th}$ GPU stats at 0 (```gpu:0``` = ```gpu```)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c8f067b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c8f067b",
        "outputId": "06ad2acf-a292-4947-dc0a-9b8443f5fcf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(device(type='cpu'),\n",
              " device(type='cuda', index=0),\n",
              " device(type='cuda', index=1))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cpu():  #@save\n",
        "    \"\"\"Get the CPU device.\"\"\"\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def gpu(i=0):  #@save\n",
        "    \"\"\"Get a GPU device.\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "\n",
        "cpu(), gpu(), gpu(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "03f59a06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03f59a06",
        "outputId": "28b609bf-bde4-42c9-bc0b-bf37cf7ec2e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# query the number of available GPUs\n",
        "def num_gpus():  #@save\n",
        "    \"\"\"Get the number of available GPUs.\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "\n",
        "num_gpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9c1b4ae2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c1b4ae2",
        "outputId": "a3de11b4-9f16-43e1-f531-001a3f8454f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0),\n",
              " device(type='cpu'),\n",
              " [device(type='cuda', index=0)])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run code if requested GPUs not exist\n",
        "def try_gpu(i=0):  #@save\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if num_gpus() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "def try_all_gpus():  #@save\n",
        "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
        "    return [gpu(i) for i in range(num_gpus())]\n",
        "\n",
        "try_gpu(), try_gpu(10), try_all_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85926da",
      "metadata": {
        "id": "c85926da"
      },
      "source": [
        "### 6.7.2. Tensors and GPUs\n",
        "By default, tensors are created on the CPU. We can query the device where the tensor is located.  \n",
        "Whenever we want to operate on multiple terms, they need to be on the same device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b76da107",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b76da107",
        "outputId": "1ef24c54-bd69-481a-9d14-de19a14bc5e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "x.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e0e1be",
      "metadata": {
        "id": "d0e0e1be"
      },
      "source": [
        "#### 6.7.2.1. Storage on the GPU\n",
        "Ways to store a tensor on the GPU:\n",
        "- specify the storage device when creating a tensor.\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d2e8220a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2e8220a",
        "outputId": "1096f0da-2d8f-4439-c7bc-98ed83329dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]], device='cuda:0')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.ones(2, 3, device=try_gpu())\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0DHrMDo813Lo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DHrMDo813Lo",
        "outputId": "b4b9f2d0-a52d-4a65-fd9e-8f124f37d3c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8111, 0.6049, 0.5527],\n",
              "        [0.8126, 0.0376, 0.5428]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# assume >= 2 GPUs: device='cuda:1'\n",
        "Y = torch.rand(2, 3, device=try_gpu(1))\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8SjdVG32dwbx",
      "metadata": {
        "id": "8SjdVG32dwbx"
      },
      "source": [
        "#### 6.7.2.2 Copying\n",
        "To compute X+Y, move X to the second GPU then compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "m4V7neS-dqDx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4V7neS-dqDx",
        "outputId": "63103db3-cc37-4297-80b0-36b5a597f4da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.8111, 1.6049, 1.5527],\n",
              "        [1.8126, 1.0376, 1.5428]], device='cuda:0')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2 GPU needed\n",
        "# Z = X.cuda(1)\n",
        "# print(X)\n",
        "# print(Z)\n",
        "\n",
        "# move Y from cpu to gpu (Z) then compute\n",
        "Z = Y.cuda(0)\n",
        "X + Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "nFn_1N3BeSNy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFn_1N3BeSNy",
        "outputId": "7f999cd0-af8f-4374-b491-cfeadb70790d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# if Z is in the GPU, if call Z.cuda(), return then Z instead of making a copy.\n",
        "Z.cuda(0) is Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oyjRgDjIfeX1",
      "metadata": {
        "id": "oyjRgDjIfeX1"
      },
      "source": [
        "#### 6.7.2.3 Side Notes\n",
        "- use GPUs to do machine learning because they expect them to be fast.\n",
        "- transferring variables between devices is slow: much slower than computation. Parallelization a lot more difficult. Copy operations must be careful.\n",
        "- several operations at a time are much better than many single operations interspersed in the code.\n",
        "- when we print tensors or convert tensors to the NumPy format, if the data is not in the main memory, the framework will copy it to the main memory first, resulting in additional transmission overhead."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oDgI9jMagPGC",
      "metadata": {
        "id": "oDgI9jMagPGC"
      },
      "source": [
        "### 6.7.3. Neural Networks and GPUs\n",
        "A neural network modeal can specify devices. When the input is in GPU, the model will calculate the result on the same GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "HP0FQ6zbfdd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP0FQ6zbfdd7",
        "outputId": "e16c6342-2e18-42b1-df2e-a6e3c826ac87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.1920],\n",
              "         [0.1920]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " device(type='cuda', index=0))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(1))\n",
        "net = net.to(device=try_gpu())\n",
        "net(X), net[0].weight.data.device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "UidXXJqngU4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "UidXXJqngU4f",
        "outputId": "7c124f1b-3c38-4ecf-f23a-2d97cdda8351"
      },
      "outputs": [],
      "source": [
        "# let the trainer support GPU\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "    self.save_hyperparameters()\n",
        "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def prepare_batch(self, batch):\n",
        "    if self.gpus:\n",
        "        batch = [a.to(self.gpus[0]) for a in batch]\n",
        "    return batch\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def prepare_model(self, model):\n",
        "    model.trainer = self\n",
        "    model.board.xlim = [0, self.max_epochs]\n",
        "    if self.gpus:\n",
        "        model.to(self.gpus[0])\n",
        "    self.model = model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34712735",
      "metadata": {},
      "source": [
        "### 6.7.4 Summary\n",
        "- By default, data is created in the main memory and then uses the CPU for calculations.\n",
        "- The deep learning framework requires all input data for calculation to be on the same device.\n",
        "- It is much better to allocate memory for logging inside the GPU and only move larger logs."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
